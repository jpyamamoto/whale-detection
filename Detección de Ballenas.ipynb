{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48a71e7-74af-4983-8caf-53ead3fb686a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detección Sonora de Ballenas Francas\n",
    "\n",
    "> Juan Pablo Yamamoto Zazueta  \n",
    "> [jpyamamoto@ciencias.unam.mx](mailto:jpyamamoto.ciencias.unam.mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ceb2da-9045-4bb8-a5be-5b324c587651",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instalación e inicialización de entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6145ec2-3901-4cd2-803c-c9e2c075a461",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d655a-720f-46b8-bd92-a0a1cf4a7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7436fd-dc63-451c-ae0a-91faa478d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae8523-b5a5-4e1e-9716-d480af6ab72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db960a8-7dc8-4400-a1be-ec7c09ea6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa44eb8-e0b1-424b-bd2d-8877674417f7",
   "metadata": {},
   "source": [
    "### Importar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabac05-0dd2-4837-a9d5-f41987308a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualización\n",
    "\n",
    "from IPython.display import Audio, display, HTML, IFrame, clear_output\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2a164-293f-4977-8e88-9480d734a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca2455-df71-4ecc-b5a9-797a4f291a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herramientas\n",
    "\n",
    "import opendatasets as od\n",
    "import librosa\n",
    "from scipy.fft import fft\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc57bd-d0fc-4148-a436-f05bb66ce075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Red Neuronal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops.focal_loss import sigmoid_focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8044f-cdb8-46f3-9e4c-1820580fea67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preparación de los datos\n",
    "\n",
    "El conjunto de datos fue desarrollado por André Karpištšenko, Eric Spaulding y Will Cukierski, a través de Kaggle en la competencia [\"The Marinexplore and Cornell University Whale Detection Challenge\"](https://kaggle.com/competitions/whale-detection-challenge).\n",
    "\n",
    "Podemos descargarlo desde Kaggle utilizando la biblioteca `opendatasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378aed8-cccc-4018-b91d-106fb3ad2957",
   "metadata": {},
   "source": [
    "**Nota:** Primero dirígete al sitio web de la competencia ([https://www.kaggle.com/competitions/whale-detection-challenge/data](https://www.kaggle.com/competitions/whale-detection-challenge/data)) y acepta el aviso de Copyright para poder descargar el conjunto de datos.\n",
    "\n",
    "Posteriormente, descarga tus credenciales para usar la API de Kaggle en la configuración de tu perfil, y coloca el archivo `kaggle.json` en el mismo directorio de este archivo (para ser detectado automáticamente) o copia y pega el contenido del archivo en la entrada que se desplegará al ejecutar la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce0ae2-fc89-4ab4-ada6-e6e893a7395a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/competitions/whale-detection-challenge/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65492d4d-f3cf-4760-b57c-db3134f8b56d",
   "metadata": {},
   "source": [
    "Procedemos a organizar los archivos en que recibimos los datos, de manera que sean fácil de explorar.\n",
    "\n",
    "La primera celda mueve los datos de ejemplo a la carpeta `sample`, y la segunda celda pone los archivos para entrenamiento y pruebas en la carpeta `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1582a-4a16-4c66-9a43-baeace3f42bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Capturamos la salida porque son demasiados archivos y ocupan mucho espacio en el notebook\n",
    "\n",
    "!unzip whale-detection-challenge/small_data_sample_revised.zip -d sample/\n",
    "!cp whale-detection-challenge/sample_submission.csv sample/\n",
    "!rm -r sample/__MACOSX\n",
    "!mv sample/small_data_sample/* sample/\n",
    "!rm -r sample/small_data_sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397fa28-df78-472b-8bfa-60bc49cf6149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Capturamos la salida porque son demasiados archivos y ocupan mucho espacio en el notebook\n",
    "\n",
    "!unzip whale-detection-challenge/whale_data.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2beb8c1-9a8b-4a4e-9062-184fa30b43ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archivos_entrenamiento = glob.glob('./data/train/*.aiff')\n",
    "archivos_prueba = glob.glob('./data/test/*.aiff')\n",
    "\n",
    "print(\"Archivos de entrenamiento: {}\".format(len(archivos_entrenamiento)))\n",
    "print(\"Archivos de test: {}\".format(len(archivos_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084b23b-00ee-4273-bc80-fb52fa6296e0",
   "metadata": {},
   "source": [
    "Para manipular los archivos de audio que se encuentran en formato `.aiff`, vamos a utilizar la biblioteca [librosa](https://librosa.org/) que incluye varias herramientas relacionadas al procesamiento de audio.\n",
    "\n",
    "Podemos explorar algunos ejemplos de las grabaciones que tiene el conjunto con los archivos contenidos en el directorio `sample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8c522-9a33-45c0-99f6-f531b9a0d3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archivos_no_whale = glob.glob('./sample/no_right_whale/*.aiff')\n",
    "samples_no_whale = [librosa.load(file, sr=None) for file in archivos_no_whale]\n",
    "\n",
    "archivos_whale = glob.glob('./sample/right_whale/*.aiff')\n",
    "samples_whale = [librosa.load(file, sr=None) for file in archivos_whale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d9453-d6d8-475f-aa06-0b12fb9293de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_whale, samplerate_nw = librosa.load('./sample/no_right_whale/train1.aiff')\n",
    "whale, samplerate_w = librosa.load('./sample/right_whale/train9.aiff')\n",
    "\n",
    "print(\"No whale:\")\n",
    "display(Audio(no_whale, rate=samplerate_nw))\n",
    "print(\"Whale:\")\n",
    "display(Audio(whale, rate=samplerate_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be43a3-cf9a-4780-bd9f-5c64404de7fd",
   "metadata": {},
   "source": [
    "Todos los audios cumplen con las siguientes especificaciones:\n",
    "\n",
    "- Duración: 2 segundos\n",
    "- Frecuencia de muestreo: 2 kHz\n",
    "\n",
    "Debemos tener cuidado al utilizar la biblioteca `librosa`, pues por defecto va a hacer un resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623b156-78ce-4363-a669-7f874bd322ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, samplerate_original = samples_no_whale[0]\n",
    "_, samplerate_default = librosa.load('./sample/no_right_whale/train1.aiff')\n",
    "\n",
    "print(\"Frecuencia de muestreo original: {} Hz\".format(samplerate_original))\n",
    "print(\"Frecuencia de muestreo por defecto: {} Hz\".format(samplerate_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8d491-d7b7-497a-86b4-e97cb87198ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Análisis de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2328c-a076-4ed8-b698-8afa02ef6287",
   "metadata": {
    "tags": []
   },
   "source": [
    "Primero que nada, para poder procesar datos de audio tenemos que entender la forma en que se representa el audio digitalmente.\n",
    "\n",
    "Lo que un micrófono haría es medir la presión relativa que ejerce el medio (normalmente aire, pero en nuestro caso el medio sería agua) sobre el dispositivo de captura. De manera tal que una mayor presión representa una onda enviada a través del medio con mayor fuerza.\n",
    "\n",
    "Por otro lado, esta presión al ser en realidad la medición de ondas, tiene otra propiedad: la frecuencia. Cuando vemos la representación de una onda, según la cercanía de dos crestas de la onda es la frecuencia del sonido que escuchamos.\n",
    "\n",
    "Si bien esta presión en el mundo real se considera una función continua, para trabajar con ella digitalmente, tenemos que discretizarla. Para esto se toman muestras en intervalos de tiempo bien definidos, y se redondean a un valor representable según la precisión con que cuenta la computadora.\n",
    "\n",
    "Esta es, a grandes rasgos, la manera en que se almacena audio de forma digital. Esto nos lleva a la primera visualización que podemos hacer de los datos que tenemos, y es la visualización por amplitud de onda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af225eca-da45-4e3e-b414-aae472a086e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(20, 9))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Amplitud de Onda: Sin Ballenas')\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio = sample[0]\n",
    "    ax.plot(audio)\n",
    "    ax.set_xlim([0, audio.shape[0]])\n",
    "    ax.set_ylim([-0.1, 0.1])\n",
    "    ax.set_xlabel('Muestra')\n",
    "    ax.set_ylabel('Amplitud')\n",
    "    \n",
    "ax6.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8061e-56b2-41e1-b3c3-0c0ebfd061d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(20, 9))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Amplitud de Onda: Con Ballenas')\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio = sample[0]\n",
    "    ax.plot(audio)\n",
    "    ax.set_xlim([0, audio.shape[0]])\n",
    "    ax.set_ylim([-0.1, 0.1])\n",
    "    ax.set_xlabel('Muestra')\n",
    "    ax.set_ylabel('Amplitud')\n",
    "    \n",
    "ax6.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872533d-e8cf-4460-b09f-9f5a9e7f3d26",
   "metadata": {},
   "source": [
    "Como mencionamos anteriormente, la frecuencia de un sonido depende de la cercanía entre dos crestas de la onda medida. Por otro lado, al discretizar la onda, únicamente podemos tomar medidas cada cierto intervalo de tiempo. Esto quiere decir que ciertas frecuencias requieren mayor cantidad de mediciones para ser capturadas apropiadamente, y otras requieren menos.\n",
    "\n",
    "La medida de cuántas muestras debemos tomar para poder recuperar de manera apropiada la frecuencia grabada en un audio, es la [frecuencia Nyquist](https://mathworld.wolfram.com/NyquistFrequency.html), la cuál indica lo siguiente:\n",
    "\n",
    "La mayor frecuencia $f_{\\text{Nyquist}}$ que puede ser capturada con una tasa de muestreo $v$, es $f(v)=\\dfrac{1}{2}v$.\n",
    "\n",
    "Eso nos da una primera pista sobre nuestros datos: si la frecuencia de muestreo es de 2 kHz, eso quiere decir que la máxima frecuencia registrada en el sonido puede ser de 1 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a5b5d-cdc2-4d43-a71e-65fee19cc77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(20, 9))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Densidad Espectral: Sin Ballenas')\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio, sr = sample\n",
    "    X = fft(audio)\n",
    "    f = np.linspace(0, sr, len(X))\n",
    "    ax.plot(f, X)\n",
    "    ax.set_xlim([0, f.shape[0]/2])\n",
    "    ax.set_xlabel('Frecuencia (Hz)')\n",
    "    ax.set_ylabel('Magnitud')\n",
    "    \n",
    "ax6.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd723c-a349-47f0-994b-81c5bff0ef0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(20, 9))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Densidad Espectral: Con Ballenas')\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio, sr = sample\n",
    "    X = fft(audio)\n",
    "    f = np.linspace(0, sr, len(X))\n",
    "    ax.plot(f, X)\n",
    "    ax.set_xlim([0, f.shape[0]/2])\n",
    "    ax.set_xlabel('Frecuencia (Hz)')\n",
    "    ax.set_ylabel('Magnitud')\n",
    "    \n",
    "ax6.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081eac6-cdc8-4ee9-99aa-0fd47eef744b",
   "metadata": {},
   "source": [
    "Podemos ver en los ejemplos que efectivamente, las frecuencias capturadas se encuentran entre 0 y 1000 Hz. Tenemos valores hasta 2 kHz por la forma en que funciona la transformada de Fourier, que refleja los resultados sobre la frecuencia de Nyquist (en nuestro caso, recordemos que era 1000).\n",
    "\n",
    "Sin embargo, esto sigue sin darnos suficiente información para detectar el sonido de las ballenas. Notemos que las gráficas son bastante parecidas en ambas categorías, a excepción de algunos casos donde las grabaciones con ballenas parecen tener magnitudes más grandes.\n",
    "\n",
    "Sin embargo, no podemos confiar en esto, pues no sabemos si esas frecuencias son causadas por las ballenas o por cualquier otro factor en la grabación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec653d-7dcb-41e7-a575-3e547320becb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conociendo a las Ballenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c321f3-fa31-45cf-b6e6-0cac16bcf74c",
   "metadata": {},
   "source": [
    "Investigando más sobre las ballenas francas y los sonidos que hacen, encontramos los siguientes datos relevantes:\n",
    "\n",
    "- Producen 3 tipos de sonidos: gemidos, quejidos y eruptos.\n",
    "- La mayoría de los sonidos que producen se mantienen por debajo de los 500 Hz.\n",
    "- Pocas veces emiten sonidos que alcanzan hasta los 4 kHz.\n",
    "- El sonido distintivo de las ballenas francas es el \"whoop\" o \"up call\":\n",
    "    - Sube de los 50 Hz a los 440 Hz.\n",
    "    - Dura alrededor de 2 segundos.\n",
    "    - Se utiliza para llamar a otras ballenas francas y generalmente las atrae a reunirse.\n",
    "    - Es un sonido que emiten de manera constante y común.\n",
    "    - Es el sonido más comúnmente utilizado por los expertos para identificar ballenas francas.\n",
    "    \n",
    "De los datos anteriores, considero que la mejor vía para detectar a las ballenas es usando el sonido \"whoop\", confiando en lo que hacen los expertos. Esto nos ayuda a identificar qué tipo de patrones buscar:\n",
    "- Incremento en la frecuencia.\n",
    "- Comienza cerca de 50 Hz y sube cerca de 440 Hz.\n",
    "- Presente en la duración de la grabación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1128c7c-44c0-4a16-b5bc-b72134747bc5",
   "metadata": {},
   "source": [
    "A continuación podemos ver una grabación donde se escucha el sonido \"whoop\". Una grabación más clara de este sonido se escucha en el siguiente video que no podemos incrustar en el notebook por restricciones impuestas por la plataforma donde se publicó: [North Atlantic Right Whale \"up call\"](https://vimeo.com/227009627)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf941009-d79b-47aa-8d2f-149bd1a129d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IFrame(src=\"https://www.youtube.com/embed/1WFnX4zO9GU\", height=315, width=560)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c4267-bb54-4a7e-8e82-a388127f7391",
   "metadata": {},
   "source": [
    "Podemos ver en los videos que se están usando espectrogramas para visualizar las frecuencias del sonido, y en ambos vemos unas curvas convexas.\n",
    "\n",
    "Esto concuerda con la imagen provista en la página del concurso, distinguiendo el sonido de la ballena franca:\n",
    "\n",
    "![Imagen Espectrograma](https://storage.googleapis.com/kaggle-competitions/kaggle/3353/media/whale_detection_challenge-cwc-es.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec6772-d81b-45a1-889d-55929cacca48",
   "metadata": {},
   "source": [
    "Si bien la visualización de amplitud de onda nos puede dar una idea de qué frecuencias colaboran a la onda durante los 2 segundos de grabación, no es suficiente conocer que hay sonidos en las frecuencias 50 - 440 kHz. También necesitamos saber que siguen el patrón ascendente. De otra manera podría ser que hay otros factores generando sonidos en esas frecuencias, sin estar relacionados.\n",
    "\n",
    "Por lo tanto, tenemos que visualizar las frecuencias no sólo por su contribución a la onda total, sino considerando también su magnitud en el tiempo. Para ello vamos a utilizar los **espectrogramas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ddf90-e10c-47b0-ab92-d0f4c2f59bdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Espectrogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f781c-ca80-4703-b221-c0429108571c",
   "metadata": {},
   "source": [
    "Para visualizar las frecuencias que componen una onda a través del dominio del tiempo, podemos usar espectrogramas. Estos van a descomponer la onda en sus frecuencias, en una cierta ventana de tiempo. Luego, va a utilizar códigos de color para mostrar la intensidad en decibelios de una cierta frecuencia en cada intervalo.\n",
    "\n",
    "Podemos visualizarlo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43e9a8-558f-4480-9e1d-dc5e231bdc96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(20, 15))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Frecuencias: Sin Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio, sr = sample\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60e7d7-34b2-4101-8c3a-05efa38d3d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(20, 15))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Frecuencias: Con Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio, sr = sample\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556c2a6-53ad-4b52-8259-5bd3349203f0",
   "metadata": {},
   "source": [
    "Como podemos ver, los espectrogramas contienen demasiada información en un comienzo, y no son precisamente informativos. Vamos a intentar limpiar un poco más los datos para ser capaces de extraer la información relevante a nuestro problema.\n",
    "\n",
    "Una primera mejora que podemos intentar aplicar es únicamente considerar las frecuencias en el rango 0 Hz - 500 Hz. Esto es porque, como vimos antes, es el rango de frecuencias que emiten las ballenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed412d8-75ba-41e5-90fa-e1ad993aac38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Frecuencias: Sin Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio, sr = sample\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd98214-2ff1-4dd1-9c3e-94c59849b35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Frecuencias: Con Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio, sr = sample\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010b55f-d6d2-465c-8f58-8ff6d46d139e",
   "metadata": {},
   "source": [
    "Ahora estamos limitando los espectrogramas a frecuencias hasta los 500 Hz. Igualmente notemos que los intervalos de tiempo son demasiado amplios. Si contamos las divisiones verticales, encontramos que solo hay 8 ventanas de tiempo en las que estamos midiendo las frecuencias.\n",
    "\n",
    "El algoritmo que utilizamos (Short Time Fourier Transform) requiere que indiquemos los siguientes parámetros:\n",
    "- Tamaño del intervalo: La cantidad de muestras que se van a considerar en la ventana a la que se aplica la transformada de Fourier.\n",
    "- Tamaño del salto: La cantidad de muestras a desplazarnos entre una ventana de tiempo y la siguiente.\n",
    "\n",
    "En realidad no hay una medida concreta sobre qué valores deben tomar esas variables. Mediante prueba y error (podemos hacer la prueba en la siguiente celda) encontramos valores que nos convencieron pues parecen entregar imágenes descriptivas, sin ser de un tamaño no manejable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd531f-7102-47e3-9608-84067a45b237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@interact(\n",
    "    frame_size = widgets.IntSlider(min=10, max=500, step=1, value=256),\n",
    ")\n",
    "def visualize_stft(frame_size):\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    hop_length = frame_size // 4\n",
    "\n",
    "    fig.suptitle('Espectrograma (Intervalo: {}, Salto: {})'.format(frame_size, hop_length))\n",
    "\n",
    "    audio_nw, sr_nw = samples_no_whale[4]\n",
    "    audio_w, sr_w = samples_whale[4]\n",
    "    \n",
    "    D_nw = librosa.amplitude_to_db(np.abs(librosa.stft(audio_nw, hop_length=hop_length, n_fft=frame_size)), ref=np.max)\n",
    "    img = librosa.display.specshow(D_nw, y_axis='linear', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr_nw, ax=ax1)\n",
    "    \n",
    "    D_w = librosa.amplitude_to_db(np.abs(librosa.stft(audio_w, hop_length=hop_length, n_fft=frame_size)), ref=np.max)\n",
    "    img = librosa.display.specshow(D_w, y_axis='linear', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr_w, ax=ax2)\n",
    "    \n",
    "    ax1.set_ylim([0, 500])\n",
    "    ax1.set_xlabel('Tiempo (segundos)')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.set_title('Sin Ballena')\n",
    "    \n",
    "    ax2.set_ylim([0, 500])\n",
    "    ax2.set_xlabel('Tiempo (segundos)')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2.set_title('Con Ballena')\n",
    "    \n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "    fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a8928-9cd1-4012-a97c-2e8797cb25b8",
   "metadata": {},
   "source": [
    "Algunos consejos para seleccionar esos valores, que encontramos son los siguientes:\n",
    "- Que el tamaño del intervalo sea una potencia de 2.\n",
    "- Que el tamaño del salto sea una cuarta parte del tamaño del intervalo.\n",
    "\n",
    "Por ello, habiendo encontrado valores que nos convencieron en la celda anterior, redondeamos el tamaño del intervalo a la potencia de 2 más cercana y dimos el tamaño del salto una cuarta parte de este.\n",
    "\n",
    "Esto nos dio los siguientes valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1f462-d47e-479d-9b4a-004c528db23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_size = 256\n",
    "hop_length = frame_size // 4\n",
    "\n",
    "print(\"Tamaño del intervalo: {}\".format(frame_size))\n",
    "print(\"Tamaño del salto: {}\".format(hop_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8eec6c-9b66-410f-8b7d-efa84e98b697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Frecuencias: Sin Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio, sr = sample\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio, hop_length=hop_length, n_fft=frame_size)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c3a86-7924-4055-9d45-723d85851a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Frecuencias: Con Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio, sr = sample\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio, hop_length=hop_length, n_fft=frame_size)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09b45a-2cab-4e57-8124-755e0cfbff9b",
   "metadata": {},
   "source": [
    "### Espectrogramas de Mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f156e-8939-40e6-a1d0-9081d99abc56",
   "metadata": {},
   "source": [
    "Hay un último hecho que queremos tomar en cuenta al momento de generar los espectrogramas. Esto es que los humanos (y en general los animales) no percibimos el sonido de forma lineal. Es decir, al reproducir dos frecuencias a un intervalo dado y luego reproducir dos notas al mismo intervalo en una frecuencia distinta (y asumamos que lejana), percibiremos claramente que un par suena más parecido que el otro.\n",
    "\n",
    "Esto se debe a que el sonido se percibe de forma logarítmica. Mientras mayor es la frecuencia, más lejanas deben estar dos frecuencias para que el intervalo entre ellas se **perciba** como constante.\n",
    "\n",
    "Puesto que esto es algo generalizado en los animales, podemos asumir que las ballenas también perciben de esta manera el sonido. Y por lo mismo parece apropiado asumir que la forma de analizar el sonido emitido por ellas debe ajustarse a esta métrica.\n",
    "\n",
    "Para ello vamos a utilizar los espectrogramas de Mel.\n",
    "\n",
    "En la imagen siguiente podemos visualizar cómo funciona el filtro que será aplicado a la imagen. Podemos ver que según la frecuencia del sonido, será la intensidad del filtro que se aplique.\n",
    "\n",
    "La tasa de muestreo al ser baja (recordemos que era de 200), no permite visualizar tan claramente la forma logarítmica que tiene este filtro, debido a que son pocas las frecuencias alcanzables. Pero podemos hacer el experimento de incrementar la variable `sr` a un valor como `22050` (la tasa de muestreo estándar para archivos de música) y va a ser clara la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf2f16-25dc-4059-97f9-596878a0e37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@interact(\n",
    "    mel_bands = widgets.IntSlider(min=5, max=50, step=1, value=10),\n",
    ")\n",
    "def visualize_stft(mel_bands):\n",
    "    fig, ax = plt.subplots()\n",
    "    sr = 200\n",
    "    filter_banks = librosa.filters.mel(n_fft=frame_size, sr=sr, n_mels=mel_bands)\n",
    "    librosa.display.specshow(filter_banks, sr=sr, x_axis='linear')\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    ax.set(title='Espectrograma de los filtros de Mel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673645d-c134-45d1-94c4-562fcd4f957f",
   "metadata": {},
   "source": [
    "Vamos a buscar el número apropiado de bandas de Mel para nuestro caso de uso. No encontré alguna fórmula o heurística para determinar este valor, sino que más bien la elección debe ir guiada por la resolución que da (y lo que esperamos ver en la imagen).\n",
    "\n",
    "En la siguiente celda podemos experimentar con el valor que consideramos da un buen resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c9ee5-906f-4a12-947a-ec6da91eda71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@interact(\n",
    "    mel_bands = widgets.IntSlider(min=10, max=200, step=1, value=128),\n",
    ")\n",
    "def visualize_stft(mel_bands):\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    fig.suptitle('Espectrograma (Bandas de Mel: {})'.format(mel_bands))\n",
    "\n",
    "    audio_nw, sr_nw = samples_no_whale[4]\n",
    "    audio_w, sr_w = samples_whale[4]\n",
    "    \n",
    "    mel_spec_nw = librosa.feature.melspectrogram(y=audio_nw, sr=sr_nw, n_fft=frame_size, hop_length=hop_length, n_mels=mel_bands)\n",
    "    mel_spec_w = librosa.feature.melspectrogram(y=audio_w, sr=sr_w, n_fft=frame_size, hop_length=hop_length, n_mels=mel_bands)\n",
    "    \n",
    "    D_nw = librosa.power_to_db(mel_spec_nw, ref=np.max)\n",
    "    img = librosa.display.specshow(D_nw, y_axis='mel', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr_nw, ax=ax1)\n",
    "    \n",
    "    D_w = librosa.power_to_db(mel_spec_w, ref=np.max)\n",
    "    img = librosa.display.specshow(D_w, y_axis='mel', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr_w, ax=ax2)\n",
    "    \n",
    "    ax1.set_ylim([0, 500])\n",
    "    ax1.set_xlabel('Tiempo (segundos)')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.set_title('Sin Ballena')\n",
    "    \n",
    "    ax2.set_ylim([0, 500])\n",
    "    ax2.set_xlabel('Tiempo (segundos)')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2.set_title('Con Ballena')\n",
    "    \n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "    fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a948b-973a-40b3-804c-95f53fe7eb08",
   "metadata": {},
   "source": [
    "El valor que nos convenció fue el de la siguiente celda.\n",
    "\n",
    "- Con valores más grandes, parece haber mucho ruido en la imagen.\n",
    "- Con valores más pequeños se desvanecen algunos colores y perdemos información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c4d54-d2e9-4d18-bfba-e9faf3d6f27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mel_bands = 128\n",
    "\n",
    "print(\"Bandas de Mel: {}\".format(mel_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd49fde-90a0-4016-8a07-8ad457d25857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Mel: Sin Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio, sr = sample\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=frame_size, hop_length=hop_length, n_mels=mel_bands)\n",
    "    D = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='mel', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b97a6-cfe8-4544-a01b-46f8791fcee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Mel: Con Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio, sr = sample\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=frame_size, hop_length=hop_length, n_mels=mel_bands)\n",
    "    D = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='mel', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccc555-a5ae-4396-bf03-cb92a26c7577",
   "metadata": {},
   "source": [
    "Finalmente, notemos que la forma en que utilizamos la escala de colores es lineal. Si bien para un humano es más fácil visualizar la intensidad por colores, la computadora lo va a leer como números de mayor o menor tamaño.\n",
    "\n",
    "Por lo tanto, podemos convertir la imagen a escala de grises para facilitar su procesamiento por la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db40fbf-abcd-40a7-89c4-5ad9ebefb83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Mel: Sin Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_no_whale):\n",
    "    audio, sr = sample\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=frame_size, hop_length=hop_length, n_mels=mel_bands)\n",
    "    D = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(D, cmap='gray', y_axis='mel', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6586549-8d27-41be-813f-b0d6168c4d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "plots = [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "fig.suptitle('Espectrograma de Mel: Con Ballenas')\n",
    "img = None\n",
    "\n",
    "for ax, sample in zip(plots, samples_whale):\n",
    "    audio, sr = sample\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=frame_size, hop_length=hop_length, n_mels=mel_bands)\n",
    "    D = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(D, cmap='gray', y_axis='mel', x_axis='time', hop_length=hop_length, n_fft=frame_size, sr=sr, ax=ax)\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.set_xlabel('Tiempo (segundos)')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    \n",
    "ax6.axis('off')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.10, 0.02, 0.8])\n",
    "fig.colorbar(img, cax=cbar_ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced3ddc-732b-4a14-b183-c190831fd456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Procesamiento de las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259367f8-d905-4c8b-8eb8-d8d8d732956b",
   "metadata": {},
   "source": [
    "Tras haber determinado la forma en que convertiremos los archivos de audio a imágenes, y encontrando la configuración adecuada, vamos a procesar los archivos para poder utilizarlos con la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84603503-0a6f-452a-b4a9-88e26709c62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def audio_to_img(audio, sr):\n",
    "    img = librosa.feature.melspectrogram(y=audio,\n",
    "                                         sr=sr,\n",
    "                                         n_fft=frame_size,\n",
    "                                         hop_length=hop_length,\n",
    "                                         n_mels=mel_bands)\n",
    "   \n",
    "    img = librosa.power_to_db(img, ref=np.max)\n",
    "    img = np.flip(img, axis=0)\n",
    "    img = np.c_[ img[64:], np.ones(64)] # Imagen cuadrada\n",
    "    \n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "    img = img * 255\n",
    "    \n",
    "    img = img.astype(np.uint8)\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "def convert_audio_file(file, output):\n",
    "    audio, sr = librosa.load(file, sr=None)\n",
    "    img = audio_to_img(audio, sr)\n",
    "    img.save(output, format='PNG', optimize=False, compress_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a487d24-c71f-43ae-9657-1f00062e99a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_labels = dict()\n",
    "\n",
    "with open('./data/train.csv', 'r') as data:\n",
    "    next(data)\n",
    "    \n",
    "    for row in csv.reader(data, delimiter=','):\n",
    "        training_labels[row[0]] = int(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33101cf9-b293-4bd3-b0ab-7d3b50e9fb39",
   "metadata": {},
   "source": [
    "Primero convertimos los archivos de audio para entrenamiento a imágenes `png` sin compresión ni perdida. Los colocaremos en los directorios:\n",
    "- `./data_img/train/negative`: Si tiene etiqueta negativa (no hay ballena presente).\n",
    "- `./data_img/train/positive`: Si tiene etiqueta positiva (sí hay ballena presente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892e26e-e5f7-4052-817e-6ed9c0811174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data_img/train/negative\n",
    "!mkdir -p data_img/train/positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c91e67-70b8-4400-b014-b30621db9012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = './data_img/train/'\n",
    "\n",
    "subpath = ['negative/', 'positive/']\n",
    "\n",
    "i = 1\n",
    "total = len(archivos_entrenamiento)\n",
    "\n",
    "for file in archivos_entrenamiento:\n",
    "    filename = os.path.basename(file)\n",
    "    file_noext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    label = training_labels[filename]\n",
    "    output_filename = base_path + subpath[label] + file_noext + '.png'\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Convirtiendo archivo {} a {}\\nArchivo {}/{}'.format(filename, file_noext + '.png', i, total))\n",
    "    \n",
    "    convert_audio_file(file, output_filename)\n",
    "    i += 1\n",
    "    \n",
    "print(\"Completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbd73f-8849-4282-bdbe-806104b25a85",
   "metadata": {},
   "source": [
    "También convertimos los archivos de audio del conjunto de prueba.\n",
    "\n",
    "Estarán en el directorio `./data_img/test/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a42a1-bee4-4cd9-a1a6-038f69e33749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data_img/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885543b3-318e-4742-a7fd-bb7e31f68841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = './data_img/test/'\n",
    "\n",
    "i = 1\n",
    "total = len(archivos_prueba)\n",
    "\n",
    "for file in archivos_prueba:\n",
    "    filename = os.path.basename(file)\n",
    "    file_noext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    output_filename = base_path + file_noext + '.png'\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Convirtiendo archivo {} a {}\\nArchivo {}/{}'.format(filename, file_noext + '.png', i, total))\n",
    "    \n",
    "    convert_audio_file(file, output_filename)\n",
    "    i += 1\n",
    "    \n",
    "print(\"Completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379aea9b-ecf8-4419-be88-124a0fa5c27a",
   "metadata": {},
   "source": [
    "Habiendo convertido todas las imágenes, tenemos los siguientes conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008af59-0ab4-4728-8f7d-193fe482f1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_train_neg = glob.glob('./data_img/train/negative/*.png')\n",
    "img_train_pos = glob.glob('./data_img/train/positive/*.png')\n",
    "img_test = glob.glob('./data_img/test/*.png')\n",
    "\n",
    "print(\"Imágenes de prueba: {}\".format(len(img_test)))\n",
    "print(\"Imágenes de entrenamiento (categoría positiva): {}\".format(len(img_train_pos)))\n",
    "print(\"Imágenes de entrenamiento (categoría negativa): {}\".format(len(img_train_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550318c3-6a5c-4aac-9eef-00cc84456811",
   "metadata": {},
   "source": [
    "### DataSets y DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd1179-3c6b-47bb-9942-409e6b4663a5",
   "metadata": {},
   "source": [
    "Vamos a crear una clase para un Dataset que tome los datos de las imágenes de ballenas y las clasifique con su etiqueta correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686383a-04c4-40c7-8146-63b46107431d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WhaleSpectrogramsDataset(Dataset):\n",
    "    def __init__(self, positive_paths, negative_paths, transform=None, target_transform=None):\n",
    "        self.imgs = list(map(lambda p: (1, p), positive_paths)) + list(map(lambda p: (1, p), negative_paths))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, img_path = self.imgs[idx]\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a387c7-2e4e-42ed-8dd4-be4cb8e40294",
   "metadata": {},
   "source": [
    "Con lo anterior, podemos crear nuestros conjuntos de datos.\n",
    "\n",
    "Para el dataset de prueba, vamos a ignorar las etiquetas entonces las tomamos como clasificadas indistintamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a2b8e-677c-4bf7-a839-a220055c7e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = WhaleSpectrogramsDataset(positive_paths=img_train_pos,\n",
    "                                         negative_paths=img_train_neg,\n",
    "                                         transform=transforms.Compose([\n",
    "                                             transforms.Resize((227,227)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.5], [0.5])\n",
    "                                         ]),\n",
    "                                         target_transform=torch.tensor)\n",
    "\n",
    "mid = len(img_test) // 2\n",
    "test_dataset = WhaleSpectrogramsDataset(positive_paths=img_test[:mid],\n",
    "                                        negative_paths=img_test[mid:],\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.Resize((227,227)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.5], [0.5])\n",
    "                                        ]),\n",
    "                                        target_transform=torch.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfca82a-2f8d-4a70-9bd7-c91f140c208e",
   "metadata": {},
   "source": [
    "Con los datasets anteriores, podemos crear un Dataloader que divida los datos en batches de cierto tamaño para el entrenamiento. Vamos a utilizar batches de tamaño 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cb260-1a67-4674-bf96-920ab623ff60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4f8eb-202b-42e2-b92e-f84f9b60d14e",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9e2bf-87f9-4f2c-a4b4-3a39cc9e2967",
   "metadata": {},
   "source": [
    "Para la red neuronal, nos estaremos basando en la arquitectura de AlexNet.\n",
    "\n",
    "Esta fue introducida por primera vez en el artículo [\"ImageNet Classification with Deep Convolutional\n",
    "Neural Networks\"](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) escrito por Alex Krizhevsky, Ilya Sutskever y Geoffrey E. Hinton. Se utilizó en la competencia ImageNet del 2012, obteniendo muy buenos resultados.\n",
    "\n",
    "Principalmente se utiliza para imágenes de tamaño 227x227, con 3 canales de color. Sin embargo, veremos que podemos adecuarla a nuestro caso de uso.\n",
    "\n",
    "La arquitectura es la siguiente (imagen de [Paras Varshney](https://github.com/blurred-machine)):\n",
    "\n",
    "![Arquitectura AlexNet](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/alexnet2.png)\n",
    "\n",
    "La imagen omite algunos detalles como lo son las capas DropOut y las activaciones ReLU, pero en la implementación tenemos cuidado de incluirlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51a374-b1f1-4bac-ae1a-b287005e46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Red(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Red, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=256*6*6, out_features=4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, single=False):\n",
    "        if single:\n",
    "            x = x[None,:]\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, epochs, data_loader, criterion, optimizer, device=None, debug=False, save_dir=None):\n",
    "        error = np.zeros(epochs)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for input_data, label in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if device:\n",
    "                    input_data = input_data.to(device)\n",
    "                    label = label.to(device)\n",
    "                \n",
    "                output = self(input_data)\n",
    "                loss = criterion(output, label)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            error[epoch] = running_loss\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if debug:\n",
    "                print('-'*40)\n",
    "                print('Epoch: {}\\nElapsed Time: {:0.2f} seconds\\nError: {}'.format(epoch, (end_time - start_time) / 60, running_loss))\n",
    "            \n",
    "            if save_dir:\n",
    "                filename = os.path.join(save_dir, 'model_{}.pt'.format(epoch))\n",
    "                torch.save(self.state_dict(), filename)\n",
    "            \n",
    "        plt.plot(error)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4c3b3-526d-470f-935f-34df0674e644",
   "metadata": {},
   "source": [
    "Para el entrenamiento, vamos a utilizar una función de pérdida llamada Focal Loss.\n",
    "\n",
    "Esta es una mejora a la entropía cruzada, que sabemos es de las mejores opciones cuando tenemos tareas de clasificación. Sin embargo, la pérdida focal va a permitir adaptarse mejor a aquellos casos que la red aún no aprende bien, en vez de poner toda su atención en los casos ya aprendidos.\n",
    "\n",
    "Esto es útil cuando el conjunto de datos de entrada no es del todo balanceado, como es nuestro caso en donde tenemos casi el triple de ejemplares de una categoría.\n",
    "\n",
    "El optimizador a utilizar en nuestro caso será Adam. La elección se basa simplemente en que, en nuestra experiencia a lo largo del curso, este tiende a comportarse bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d7a6c-40a0-4227-a5ec-b6a3113551ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = Red(num_classes=2)\n",
    "\n",
    "criterion = lambda output, label: sigmoid_focal_loss(output, F.one_hot(label, num_classes=2).to(torch.float), reduction='mean')\n",
    "optimizer = optim.Adam(red.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91ff43-f5e2-40f3-82ce-881915a3d20c",
   "metadata": {},
   "source": [
    "Vamos a configurar CUDA cuando se encuentre disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ade689-1c60-4898-8f36-b8147143d703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA disponible:\", cuda)\n",
    "\n",
    "if cuda:\n",
    "    # Descomentar y cambiar el dispositivo, en caso de desearlo.\n",
    "    # torch.cuda.set_device(1)\n",
    "    \n",
    "    cuda_id = torch.cuda.current_device()\n",
    "    print(\"Dispositivo CUDA:\", torch.cuda.get_device_name(cuda_id))\n",
    "    \n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "red.to(device)\n",
    "\n",
    "print(\"Utilizando:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa88da-67c8-4446-b279-152900bc065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "red.train(epochs=20, data_loader=train_loader, criterion=criterion, optimizer=optimizer, device=device, debug=True, save_dir='./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ab672-34e3-4793-9108-ea4dba3653fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
